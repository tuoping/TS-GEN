{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31749741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(\n",
    "    sim_ckpt=\"workdir/TPS_Transition1x-rcut12-path_linearOT/PotentialEnergy/epoch=198-step=1894990.ckpt\",\n",
    "    data_dir=\"data/RGD1/\",\n",
    "    suffix=\"\",\n",
    "    num_rollouts=2,\n",
    "    # out_dir=\"./test/Transition1x-rcut12-path_linearOT/\",\n",
    "    out_dir=\"./test/Transition1x-rcut12-path_linearOT/RGD1_linked_reactions/cyclobutane_cyclization/\",\n",
    "    num_frames=1,\n",
    "    localmask=False,\n",
    "    tps_condition=True,\n",
    "    sim_condition=False\n",
    "    )\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525723df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, tqdm, time\n",
    "import numpy as np\n",
    "from mdgen.equivariant_wrapper import EquivariantMDGenWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75679b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(args.out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69697478",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mdgen.dataset import EquivariantTransformerDataset_Transition1x\n",
    "dataset = EquivariantTransformerDataset_Transition1x(data_dirname=args.data_dir, sim_condition=args.sim_condition, tps_condition=args.tps_condition, num_species=5, stage=\"cyclobutane_cyclization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c7b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a10fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(args.sim_ckpt, weights_only=False)\n",
    "hparams = ckpt[\"hyper_parameters\"]\n",
    "hparams['args'].guided = False\n",
    "# hparams['args'].sampling_method = 'euler'\n",
    "# hparams['args'].guidance_pref = 2\n",
    "hparams['args'].inference_steps = 50\n",
    "model = EquivariantMDGenWrapper(**hparams)\n",
    "print(model.model)\n",
    "model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f99428",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ckpt[\"hyper_parameters\"])\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = ckpt[\"hyper_parameters\"]['args'].embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86fb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ckpt[\"hyper_parameters\"]['args'].num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d36015",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    ")\n",
    "sample_batch = next(iter(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c014070",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[499][\"x\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6713b1",
   "metadata": {},
   "source": [
    "## Test generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d143595",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['species', 'x', 'cell', 'num_atoms', 'mask', 'v_mask', 'species_next', 'x_next', \"TKS_mask\", \"TKS_v_mask\"]:\n",
    "    try:\n",
    "        sample_batch[key] = sample_batch[key].to(device)\n",
    "    except:\n",
    "        print(f\"{key} not found\")\n",
    "\n",
    "\n",
    "pred_pos = model.inference(sample_batch)\n",
    "\n",
    "'''\n",
    "model.stage = \"inference\"\n",
    "prep = model.prep_batch(sample_batch)\n",
    "B,T,L,_ = prep[\"latents\"].shape\n",
    "t = torch.ones((B,), device=prep[\"latents\"].device)\n",
    "print(model.potential_model(prep['latents'], t, **prep['model_kwargs']).sum(dim=2).squeeze(-1)[:,1])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea204d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def rollout(model, batch):\n",
    "    expanded_batch = batch\n",
    "    \n",
    "    positions, _ = model.inference(expanded_batch)\n",
    "\n",
    "    # mask_act_space = (batch[\"mask\"] != 0)\n",
    "    # positions = positions*mask_act_space\n",
    "    new_batch = {**batch}\n",
    "    new_batch['x'] = positions\n",
    "    return positions, new_batch\n",
    "\n",
    "\n",
    "map_to_chemical_symbol = {\n",
    "    0: \"H\",\n",
    "    1: 'C',\n",
    "    2: \"N\",\n",
    "    3: \"O\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a31e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a24fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_rollouts = np.random.choice(len(dataset), size=1, replace=False)\n",
    "idx_rollouts = np.arange(len(dataset))\n",
    "print(idx_rollouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cdd6cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd14062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import Atoms\n",
    "from ase.geometry.geometry import get_distances\n",
    "import shutil, os\n",
    "from ase.io import write\n",
    "\n",
    "all_rollout_atoms_ref_0 = []\n",
    "all_rollout_atoms = []\n",
    "all_rollout_atoms_ref = []\n",
    "start = time.time()\n",
    "\n",
    "# idx_rollouts = np.arange(643, len(dataset))\n",
    "for i_rollout, idx in enumerate(idx_rollouts):\n",
    "# idx = idx_rollouts[0]\n",
    "# for i_rollout in range(args.num_rollouts):\n",
    "    item = dataset.__getitem__(idx)\n",
    "    batch = next(iter(torch.utils.data.DataLoader([item])))\n",
    "\n",
    "    for key in ['species', 'x', 'cell', 'num_atoms', 'mask', 'v_mask', 'species_next', 'x_next', \"TKS_mask\", \"TKS_v_mask\"]:\n",
    "        try:\n",
    "            batch[key] = batch[key].to(device)\n",
    "        except:\n",
    "            print(f\"{key} not found\")\n",
    "\n",
    "    labels = torch.argmax(batch[\"species\"], dim=3).squeeze(0)\n",
    "    symbols = [[map_to_chemical_symbol[int(i_elem.to('cpu'))] for i_elem in labels[i_conf]] for i_conf in range(len(labels))]\n",
    "\n",
    "    pred_pos, _ = rollout(model, batch)\n",
    "    # print(\"idx = \", idx, \"rollout\", i_rollout, pred_pos.shape)\n",
    "\n",
    "    all_atoms = []\n",
    "    all_atoms_ref = []\n",
    "    all_atoms_ref_0 = []\n",
    "    for t in range(len(pred_pos[0])):\n",
    "        print(\"rollout\", i_rollout, \"idx = \", idx, \"t\", t)\n",
    "        formula = \"\".join(symbols[t])\n",
    "\n",
    "        # print(\"t=\",t)\n",
    "        # for i in range(pred_pos.shape[2]):\n",
    "        #     err = get_distances(batch[\"x_next\"][0][t][i].cpu().numpy(), (pred_pos[0][t].cpu().numpy()[i]), cell=batch['cell'][0][0].cpu().numpy(), pbc=True)[1][0][0]\n",
    "        #     if err>0.1:\n",
    "        #         print(pred_pos[0][t].cpu().numpy()[i], batch[\"x_next\"][0][t][i].cpu().numpy(), err, err>0.1)\n",
    "        atoms = Atoms(formula, positions=pred_pos[0][t].cpu().numpy(), cell=batch['cell'][0][0].cpu().numpy(), pbc=[1,1,1])\n",
    "        # atoms.set_chemical_symbols(symbols[t])\n",
    "        all_atoms.append(atoms)\n",
    "        if args.sim_condition:\n",
    "            atoms_ref_0 = Atoms(formula, positions=batch[\"x\"][0][t].cpu().numpy(), cell=batch['cell'][0][0].cpu().numpy(), pbc=[1,1,1])\n",
    "            atoms_ref = Atoms(formula, positions=batch[\"x_next\"][0][t].cpu().numpy(), cell=batch['cell'][0][0].cpu().numpy(), pbc=[1,1,1])\n",
    "        else:\n",
    "            atoms_ref = Atoms(formula, positions=batch[\"x\"][0][t].cpu().numpy(), cell=batch['cell'][0][0].cpu().numpy(), pbc=[1,1,1])\n",
    "        all_atoms_ref.append(atoms_ref)\n",
    "        if args.sim_condition:\n",
    "            all_atoms_ref_0.append(atoms_ref_0)\n",
    "        if args.tps_condition:\n",
    "            if t == 1:\n",
    "                err = pred_pos[0][t]-batch[\"x\"][0][t]\n",
    "                print(torch.abs(err).max(), torch.abs(err).min(), torch.abs(err).mean(), )\n",
    "                assert not torch.allclose(pred_pos[0][t], batch[\"x\"][0][t])\n",
    "                assert not np.allclose(pred_pos[0][t].cpu().numpy(), batch[\"x\"][0][t].cpu().numpy())\n",
    "            else:\n",
    "                assert torch.allclose(pred_pos[0][t], batch[\"x\"][0][t])\n",
    "    # all_rollout_atoms.append(all_atoms)\n",
    "    # all_rollout_atoms_ref.append(all_atoms_ref)\n",
    "    # if args.sim_condition:\n",
    "    #     all_rollout_atoms_ref_0.append(all_atoms_ref_0)\n",
    "    out_dir = args.out_dir\n",
    "    dirname = os.path.join(out_dir, f\"rollout_{i_rollout}\")\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "\n",
    "    with open(os.path.join(dirname, \"README.md\"), \"w\") as fp:\n",
    "        fp.write(\"Data index from Transition1x: %d\"%idx)\n",
    "    filename = os.path.join(dirname, \"gentraj_1.xyz\")\n",
    "    filename_ref = os.path.join(dirname, \"reftraj_1.xyz\")\n",
    "    print(filename_ref)\n",
    "    if os.path.exists(filename):\n",
    "    #     shutil.move(filename_0, os.path.join(dirname, \"bck.0.gentraj_0.xyz\"))\n",
    "        os.remove(filename)\n",
    "    #     shutil.move(filename_ref_0, os.path.join(dirname, \"bck.0.reftraj_0.xyz\"))\n",
    "        os.remove(filename_ref)\n",
    "    assert not np.allclose(all_atoms[1].positions, all_atoms_ref[1].positions)\n",
    "    for atoms in all_atoms:\n",
    "        atoms.set_cell(np.eye(3,3)*25)\n",
    "        write(filename, atoms, append=True)\n",
    "    for ref_atoms in all_atoms_ref:\n",
    "        ref_atoms.set_cell(np.eye(3,3)*25)\n",
    "        write(filename_ref, ref_atoms, append=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6204f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "17.4/287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "for i_rollout in range(10):\n",
    "    print(\"rollout\", i_rollout, pred_pos.shape)\n",
    "    all_atoms = all_rollout_atoms[i_rollout]\n",
    "    all_atoms_ref = all_rollout_atoms_ref[i_rollout]\n",
    "    for t in range(len(pred_pos[0])):\n",
    "        print(\"t=\",t)\n",
    "        atoms = all_atoms[t]\n",
    "        atoms_ref = all_atoms_ref[t]\n",
    "        for i in range(atoms.positions.shape[0]):\n",
    "            err = get_distances(atoms_ref.positions[i], atoms.positions[i], cell=atoms.cell, pbc=True)[1][0][0]\n",
    "\n",
    "            if err>0.1:\n",
    "                print(atoms.positions[i], atoms_ref.positions[i], err, err>0.1)\n",
    "        \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odefed_mdgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
