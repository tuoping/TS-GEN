{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdgen.parsing import parse_train_args\n",
    "\n",
    "from mdgen.logger import get_logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "import torch, os\n",
    "from mdgen.dataset import LatentDataset\n",
    "\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "# args = parse_train_args()\n",
    "    \n",
    "\n",
    "trainset = LatentDataset(\"test/rcut3.5_energy_encodedim4_perturbeddata/encoded_dataset-clean/\", 3.5, num_frames=1, stage=\"train\", random_starting_point=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(trainset)):\n",
    "    print(i, trainset[i]['v'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    ")\n",
    "sample_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check latent flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from argparse import Namespace\n",
    "args_latentFlow = Namespace(\n",
    "    sim_ckpt=\"workdir/vlatent-NoiseAwareModel-path_linearOT/epoch=2659-step=891100.ckpt\",\n",
    "    data_dir=\"test/rcut3.5_energy_encodedim4_perturbeddata/encoded_dataset-clean/\",\n",
    "    suffix=\"\",\n",
    "    num_frames=1,\n",
    "    num_rollouts=1,\n",
    "    # out_dir=\"./test/localmask_rcut3.5_loss_regress-path_linearOT\",\n",
    "    out_dir=\"./test/rcut3.5_decoder_embeddim4_perturbeddata\",\n",
    "    random_starting_point=True,\n",
    "    localmask=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(args_latentFlow.out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdgen.latentflow_wrapper import LatentGenWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "ckpt = torch.load(args_latentFlow.sim_ckpt, weights_only=False)\n",
    "model_latentFlow = LatentGenWrapper(**ckpt[\"hyper_parameters\"])\n",
    "model_latentFlow.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "model_latentFlow.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['v', 'h']:\n",
    "    sample_batch[key] = sample_batch[key].to(device)\n",
    "# prep_latentFlow = model_latentFlow.prep_batch(sample_batch)\n",
    "\n",
    "pred_latent, _ = model_latentFlow.inference(sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_batch['h'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(2,2,figsize=(7, 7))\n",
    "axes[0][0].scatter(sample_batch['v'][0,0,:,0,0].detach().numpy(), pred_latent[0,0,:,0,1].detach().numpy(), s=1)\n",
    "axes[0][0].scatter(sample_batch['v'][0,0,:,1,0].detach().numpy(), pred_latent[0,0,:,1,1].detach().numpy(), s=1)\n",
    "axes[0][0].scatter(sample_batch['v'][0,0,:,2,0].detach().numpy(), pred_latent[0,0,:,2,1].detach().numpy(), s=1)\n",
    "axes[0][0].scatter(sample_batch['v'][0,0,:,3,0].detach().numpy(), pred_latent[0,0,:,3,1].detach().numpy(), s=1)\n",
    "axes[0][0].set_xlabel(\"v\")\n",
    "axes[0][0].set_ylabel(\"$v^{pred}$\")\n",
    "axes[0][0].set_title(\"$v_x$\")\n",
    "axes[0][1].scatter(sample_batch['v'][0,0,:,0,1].detach().numpy(), pred_latent[0,0,:,0,2].detach().numpy(), s=1)\n",
    "axes[0][1].scatter(sample_batch['v'][0,0,:,1,1].detach().numpy(), pred_latent[0,0,:,1,2].detach().numpy(), s=1)\n",
    "axes[0][1].scatter(sample_batch['v'][0,0,:,2,1].detach().numpy(), pred_latent[0,0,:,2,2].detach().numpy(), s=1)\n",
    "axes[0][1].scatter(sample_batch['v'][0,0,:,3,1].detach().numpy(), pred_latent[0,0,:,3,2].detach().numpy(), s=1)\n",
    "axes[0][1].set_xlabel(\"v\")\n",
    "axes[0][1].set_ylabel(\"$v^{pred}$\")\n",
    "axes[0][1].set_title(\"$v_y$\")\n",
    "axes[1][0].scatter(sample_batch['v'][0,0,:,0,2].detach().numpy(), pred_latent[0,0,:,0,3].detach().numpy(), s=1)\n",
    "axes[1][0].scatter(sample_batch['v'][0,0,:,1,2].detach().numpy(), pred_latent[0,0,:,1,3].detach().numpy(), s=1)\n",
    "axes[1][0].scatter(sample_batch['v'][0,0,:,2,2].detach().numpy(), pred_latent[0,0,:,2,3].detach().numpy(), s=1)\n",
    "axes[1][0].scatter(sample_batch['v'][0,0,:,3,2].detach().numpy(), pred_latent[0,0,:,3,3].detach().numpy(), s=1)\n",
    "axes[1][0].set_xlabel(\"v\")\n",
    "axes[1][0].set_ylabel(\"$v^{pred}$\")\n",
    "axes[1][0].set_title(\"$v_x$\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2,figsize=(8, 7))\n",
    "sc0 = axes[0][0].scatter(sample_batch['v'][0,0,:,0,0].detach().numpy(), pred_latent[0,0,:,0,1].detach().numpy(), s=1, c=sample_batch['x'][0,0,:,0])\n",
    "plt.colorbar(sc0, ax=axes[0][0])\n",
    "sc1 = axes[0][1].scatter(sample_batch['v'][0,0,:,1,0].detach().numpy(), pred_latent[0,0,:,1,1].detach().numpy(), s=1, c=sample_batch['x'][0,0,:,0])\n",
    "plt.colorbar(sc1, ax=axes[0][1])\n",
    "sc2 = axes[1][0].scatter(sample_batch['v'][0,0,:,2,0].detach().numpy(), pred_latent[0,0,:,2,1].detach().numpy(), s=1, c=sample_batch['x'][0,0,:,0])\n",
    "plt.colorbar(sc2, ax=axes[1][0])\n",
    "sc3 = axes[1][1].scatter(sample_batch['v'][0,0,:,3,0].detach().numpy(), pred_latent[0,0,:,3,1].detach().numpy(), s=1, c=sample_batch['x'][0,0,:,0])\n",
    "plt.colorbar(sc3, ax=axes[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1,1,figsize=(3.5, 3.5))\n",
    "axes.scatter(sample_batch['h'][0,0,:,0].detach().numpy(), pred_latent[0,0,:,0,0].detach().numpy(), s=1)\n",
    "axes.scatter(sample_batch['h'][0,0,:,1].detach().numpy(), pred_latent[0,0,:,1,0].detach().numpy(), s=1)\n",
    "axes.scatter(sample_batch['h'][0,0,:,2].detach().numpy(), pred_latent[0,0,:,2,0].detach().numpy(), s=1)\n",
    "axes.scatter(sample_batch['h'][0,0,:,3].detach().numpy(), pred_latent[0,0,:,3,0].detach().numpy(), s=1)\n",
    "axes.set_xlabel(\"h\")\n",
    "axes.set_ylabel(\"$h^{pred}$\")\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from argparse import Namespace\n",
    "args = Namespace(\n",
    "    sim_ckpt=\"workdir/rcut3.5_decoder_embeddim4_perturbeddata/epoch=619-step=207700.ckpt\",\n",
    "    # sim_ckpt=\"workdir/rcut3.5_energy_encodedim4/epoch=739-step=247900.ckpt\",\n",
    "    data_dir=\"test/rcut3.5_energy_encodedim4_perturbeddata/encoded_dataset-clean/\",\n",
    "    suffix=\"\",\n",
    "    num_frames=1,\n",
    "    num_rollouts=1,\n",
    "    # out_dir=\"./test/localmask_rcut3.5_loss_regress-path_linearOT\",\n",
    "    out_dir=\"./test/rcut3.5_decoder_embeddim4_perturbeddata\",\n",
    "    random_starting_point=True,\n",
    "    localmask=False,\n",
    "    )\n",
    "\n",
    "device='cuda'\n",
    "from mdgen.decoder_wrapper import DecoderWrapper\n",
    "ckpt = torch.load(args.sim_ckpt, weights_only=False)\n",
    "model = DecoderWrapper(**ckpt[\"hyper_parameters\"])\n",
    "print(model.model)\n",
    "model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "model.eval().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_h = sample_batch[\"h\"] # pred_latent[:,:,:,:,0]\n",
    "pred_v = sample_batch['v']\n",
    "pred_v = pred_latent[:,:,:,:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = model.model(pred_h.to(device), pred_v.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def lattice_vectors(a, b, c, alpha, beta, gamma):\n",
    "    # Convert angles to radians\n",
    "    alpha_rad = np.radians(alpha)\n",
    "    beta_rad = np.radians(beta)\n",
    "    gamma_rad = np.radians(gamma)\n",
    "    \n",
    "    # Calculate vector components\n",
    "    a_vec = np.array([a, 0, 0])\n",
    "    b_vec = np.array([b * np.cos(gamma_rad), b * np.sin(gamma_rad), 0])\n",
    "    \n",
    "    cx = c * np.cos(beta_rad)\n",
    "    cy = c * (np.cos(alpha_rad) - np.cos(beta_rad) * np.cos(gamma_rad)) / np.sin(gamma_rad)\n",
    "    cz = np.sqrt(c**2 - cx**2 - cy**2)\n",
    "    c_vec = np.array([cx, cy, cz])\n",
    "    \n",
    "    return a_vec, b_vec, c_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = lattice_vectors(out_dict['cell'][0,0,0].item(), out_dict['cell'][0,0,1].item(), out_dict['cell'][0,0,2].item(), out_dict['cell'][0,0,3].item(), out_dict['cell'][0,0,4].item(), out_dict['cell'][0,0,5].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_dict['cell'][0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "map_to_chemical_symbol = {\n",
    "    0: \"Cr\",\n",
    "    1: 'Co',\n",
    "    2: \"Ni\"\n",
    "}\n",
    "aatype = out_dict['aatype'][0,0].argmax(dim=-1).cpu().numpy()\n",
    "symbols = [map_to_chemical_symbol[i] for i in aatype]\n",
    "print(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_pos = out_dict['frac_pos'][0,0].detach().cpu().numpy()\n",
    "pos = frac_pos @ np.array(cell)\n",
    "print(pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frac_pos)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"\".join(symbols)\n",
    "from ase import Atoms\n",
    "atoms = Atoms(formula, positions=pos, cell=np.eye(3), pbc=True)\n",
    "from ase.io import write\n",
    "write(f\"{args.out_dir}/test.xyz\", atoms, format=\"xyz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.model.parameters(), lr=0.001)\n",
    "loss_epochs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.transport.model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    Step = 0\n",
    "    # for batch in train_loader:\n",
    "    #     # Forward pass\n",
    "    #     batch_ = {key: value.to(device) if isinstance(value, torch.Tensor) else value \n",
    "    #             for key, value in batch.items()}\n",
    "    #     prep = model.prep_batch(batch_)\n",
    "    # out_dict = model.transport.training_losses(\n",
    "    #     model=model.model,\n",
    "    #     x1=prep['latents'],\n",
    "    #     aatype1=prep['species'],\n",
    "    #     mask=prep['loss_mask'],\n",
    "    #     model_kwargs=prep['model_kwargs']\n",
    "    # )\n",
    "    # loss = out_dict[\"loss\"].mean()\n",
    "    t = torch.ones(batch_size)\n",
    "    out_dict = model.model(prep[\"latents\"], t, **prep[\"model_kwargs\"] )\n",
    "    loss = ((energy - prep[\"E\"])**2).mean()\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    # print(\"Starting backward\")\n",
    "    loss.backward()\n",
    "    # print(\"Done\")\n",
    "    optimizer.step()\n",
    "    Step += 1\n",
    "    loss_epochs.append(loss.item())\n",
    "    # print(f'Epoch [{epoch+1}/{num_epochs}], Step [{Step}], Loss: {loss.item():.4f}')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(4,3.5))\n",
    "plt.plot(loss_epochs)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.semilogy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, x0, x1 = model.transport.sample(prep['latents'])\n",
    "t, xt, ut = model.transport.path_sampler.plan(t, x0, x1)\n",
    "out_dict = model.transport.training_losses(\n",
    "    model=model.model,\n",
    "    x1=prep['latents'],\n",
    "    aatype1=prep['species'],\n",
    "    mask=prep['loss_mask'],\n",
    "    model_kwargs=prep['model_kwargs']\n",
    ")\n",
    "loss_last = out_dict[\"loss_continuous\"][torch.where(prep['loss_mask'].squeeze(-1)!=0)].detach().mean(dim=1)\n",
    "loss_fkl_last = out_dict[\"loss_fisherreg\"]\n",
    "print(loss_last)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(4,3.5))\n",
    "plt.scatter(t.detach().cpu().numpy(), loss_last.detach().cpu().numpy())\n",
    "plt.scatter(t.detach().cpu().numpy(), loss_fkl_last.detach().cpu().numpy())\n",
    "plt.xlabel(\"Diffusion time\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3.5))\n",
    "plt.scatter(t.detach().cpu().numpy(), loss_last.detach().cpu().numpy())\n",
    "plt.xlabel(\"Diffusion time\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odefed_mdgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
