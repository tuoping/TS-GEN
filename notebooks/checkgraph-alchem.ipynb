{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdgen.parsing import parse_train_args\n",
    "\n",
    "from mdgen.logger import get_logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "import torch, os\n",
    "from mdgen.dataset import EquivariantTransformerDataset_CrCoNi\n",
    "\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "# args = parse_train_args()\n",
    "    \n",
    "trainset = EquivariantTransformerDataset_CrCoNi(\"data/alchem_CrCoNi_data\", 2.5, num_frames=3, stage=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sample_traj = []\n",
    "for i in range(5):\n",
    "    sample_traj.append(trainset[i])\n",
    "\n",
    "for i in range(5):\n",
    "    print(sample_traj[i][\"species\"].shape)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# call at stage: save \n",
    "num_data = len(trainset)\n",
    "for i in range(num_data):\n",
    "    len_traj = trainset[i]\n",
    "    print(i, num_data, len_traj)\n",
    "    # print(sample_traj.keys())\n",
    "    # print(sample_traj[\"x\"].shape)\n",
    "    # print((sample_traj[\"cell\"]).shape)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "sample_batch = next(iter(train_loader))\n",
    "sample_batch = {key: value.to(device) if isinstance(value, torch.Tensor) else value \n",
    "                for key, value in sample_batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdgen.parsing import parse_train_args\n",
    "\n",
    "from mdgen.logger import get_logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "import torch, os, wandb\n",
    "from mdgen.dataset import MDGenDataset\n",
    "from mdgen.wrapper import NewMDGenWrapper\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, ModelSummary\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "# args = parse_train_args()\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(ckpt=None, validate=False, num_workers=4, epochs=10000, overfit=False, overfit_peptide=None, overfit_frame=False, train_batches=None, val_batches=None, val_repeat=25, inference_batches=0, val_freq=None, val_epoch_freq=1, no_validate=False, designability_freq=1, print_freq=100, ckpt_freq=40, wandb=False, run_name='default', accumulate_grad=1, grad_clip=1.0, check_grad=False, grad_checkpointing=False, adamW=False, ema=False, ema_decay=0.999, lr=0.0001, precision='32-true', train_split='splits/4AA_train.csv', val_split='splits/4AA_val.csv', data_dir='data/4AA_data/', num_frames=100, crop=4, suffix=\"\", atlas=False, copy_frames=False, no_pad=False, short_md=False, design_key_frames=False, no_aa_emb=False, no_torsion=False, no_design_torsion=False, supervise_no_torsions=False, supervise_all_torsions=False, no_offsets=False, no_frames=False, hyena=False, no_rope=False, dropout=0.0, scale_factor=1.0, interleave_ipa=False, prepend_ipa=True, oracle=False, num_layers=5, embed_dim=48, mha_heads=16, ipa_heads=4, ipa_head_dim=32, ipa_qk=8, ipa_v=8, time_multiplier=100.0, abs_pos_emb=True, abs_time_emb=False, prediction='velocity', sampling_method='dopri5', alpha_max=8, discrete_loss_weight=0.5, dirichlet_flow_temp=1.0, allow_nan_cfactor=False, tps_condition=True, design_from_traj=False, sim_condition=False, inpainting=False, dynamic_mpnn=False, mpnn=False, frame_interval=None, cond_interval=None)\n",
    "\n",
    "args.sim_condition=True\n",
    "args.data_dir=\"data/alchem_CrCoNi_data\" \n",
    "args.num_frames=3\n",
    "args.prepend_ipa=True \n",
    "args.abs_pos_emb=True \n",
    "args.crop=4 \n",
    "args.ckpt_freq = 40 \n",
    "args.val_repeat = 25 \n",
    "args.epochs = 10\n",
    "args.num_species = 20\n",
    "args.edge_dim = 8\n",
    "args.num_convs = 5\n",
    "args.num_heads = 4\n",
    "args.ff_dim = 8\n",
    "args.cutoff=2.5\n",
    "args.design=True\n",
    "args.path_type=\"Linear\"\n",
    "args.batch_size = batch_size\n",
    "\n",
    "os.environ[\"MODEL_DIR\"] = os.path.join(\"workdir\", args.run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdgen.equivariant_wrapper import EquivariantMDGenWrapper\n",
    "model = EquivariantMDGenWrapper(args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model.iter_step += 1\n",
    "model.stage = \"train\"\n",
    "start1 = time.time()\n",
    "prep = model.prep_batch(sample_batch)\n",
    "start = time.time()\n",
    "print(\"prep time:\", start-start1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prep.keys())\n",
    "print(prep[\"model_kwargs\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdgen.model.utils.data_utils import (\n",
    "    frac_to_cart_coords_with_lattice,\n",
    "    get_pbc_distances,\n",
    "    lattice_params_to_matrix_torch,\n",
    "    radius_graph_pbc,\n",
    ")\n",
    "from mdgen.model.gemnet.utils import (\n",
    "    repeat_blocks,\n",
    ")\n",
    "otf_graph = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prep['model_kwargs']['num_atoms'][0])\n",
    "print(prep['latents'].shape)\n",
    "print(prep['loss_mask'].shape)\n",
    "print(prep['model_kwargs']['cell'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test transport algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = prep[\"latents\"]\n",
    "aatype1=prep[\"species\"]\n",
    "mask=prep[\"loss_mask\"]\n",
    "model_kwargs=prep[\"model_kwargs\"]\n",
    "transport = model.transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, x0, x1 = transport.sample(x1)\n",
    "# t, xt, ut = transport.path_sampler.plan(t, x0, x1)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_output = model.model(xt,t,**model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdgen.transport.transport import t_to_alpha\n",
    "seq_one_hot = aatype1\n",
    "alphas, _ = t_to_alpha(t, transport.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = torch.ones_like(seq_one_hot) + seq_one_hot * (alphas[:, None, None, None] - torch.ones_like(seq_one_hot))\n",
    "x_d = torch.distributions.Dirichlet(alphas).sample()\n",
    "xt = x_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(alphas[0][0][:5]) # Batch = 0, Timestep = 0, Atoms = 0~5\n",
    "# print(xt[0][0][:5]) # Batch = 0, Timestep = 0, Atoms = 0~5\n",
    "print(xt.shape)\n",
    "print(alphas.shape)\n",
    "print(t.shape)\n",
    "print(xt.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test equivariant_latent_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model.model(xt, t, **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(model_output))\n",
    "print(model_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_dict = model.transport.training_losses(\n",
    "    model=model.model,\n",
    "    x1=prep['latents'],\n",
    "    aatype1=prep['species'],\n",
    "    mask=prep['loss_mask'],\n",
    "    model_kwargs=prep['model_kwargs']\n",
    ")\n",
    "print(out_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aatype1.argmax(dim=-1).shape)\n",
    "print(out_dict[\"logits\"].shape)\n",
    "print(out_dict[\"logits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_probs = torch.nn.functional.log_softmax(out_dict[\"logits\"], dim=-1)\n",
    "print(torch.nn.functional.cross_entropy(out_dict[\"logits\"].reshape(-1,5), aatype1.argmax(dim=-1).reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    Step = 0\n",
    "    for batch in train_loader:\n",
    "        # Forward pass\n",
    "        batch_ = {key: value.to(device) if isinstance(value, torch.Tensor) else value \n",
    "                for key, value in batch.items()}\n",
    "        prep = model.prep_batch(batch_)\n",
    "        out_dict = model.transport.training_losses(\n",
    "            model=model.model,\n",
    "            x1=prep['latents'],\n",
    "            aatype1=prep['species'],\n",
    "            mask=prep['loss_mask'],\n",
    "            model_kwargs=prep['model_kwargs']\n",
    "        )\n",
    "        loss = out_dict[\"loss\"].mean()\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        Step += 1\n",
    "        # print(f'Epoch [{epoch+1}/{num_epochs}], Step [{Step}], Loss: {loss.item():.4f}')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odefed_mdgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
